{% set name = "gptcache" %}
{% set version = "0.1.43" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: cebe7ec5e32a3347bf839e933a34e67c7fcae620deaa7cb8c6d7d276c8686f1a
  patches:                                   # [win]
    - patches/0001-read-file-as-utf8.patch   # [win]

build:
  number: 0
  # openai isn't available on s390x.
  skip: True # [py<38 or s390x]
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  entry_points:
    - gptcache_server = gptcache_server.server:main

requirements:
  build:               # [win]
    - m2-patch         # [win]
  host:
    - python
    - pip
    - wheel
    - setuptools
  run:
    - python
    - numpy
    - requests
    - cachetools
  run_constrained:
    # openai isn't mentioned in requirements.txt
    # but its import hardcoded as 'pip install openai==0.28.1' through import_openai()
    # see https://github.com/zilliztech/GPTCache/blob/0.1.43/gptcache/embedding/openai.py#L8-L10
    # because gptcache doesn't have support for openai >=1,
    # see also https://github.com/zilliztech/GPTCache/issues/576
    - openai <1.0.0a0

test:
  imports:
    - gptcache
  requires:
    - pip
  commands:
    - pip check

about:
  home: https://zilliz.com/what-is-gptcache
  summary: GPTCache is a project dedicated to building a semantic cache for storing LLM responses.
  description: |
    GPTCache is a powerful caching library that can be used to speed up and lower the cost of
    chat applications that rely on the LLM service. GPTCache works as a memcache for
    AIGC applications, similar to how Redis works for traditional applications.
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/zilliztech/GPTCache
  doc_url: https://gptcache.readthedocs.io

extra:
  recipe-maintainers:
    - YYYasin19
