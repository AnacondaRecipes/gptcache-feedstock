{% set name = "gptcache" %}
{% set version = "0.1.44" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: d3d5e6a75c57594dc58212c2d6c53a7999c23ede30e0be66d213d885c0ad0be9
  patches:                                   # [win]
    - patches/0001-read-file-as-utf8.patch   # [win]

build:
  number: 0
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  entry_points:
    - gptcache_server = gptcache_server.server:main
  skip: True # [py<38]

requirements:
  build:               # [win]
    - m2-patch         # [win]
  host:
    - python
    - pip
    - wheel
    - setuptools
  run:
    - python
    - numpy
    - requests
    - cachetools
  run_constrained:
    # openai isn't mentioned in requirements.txt
    # but its import hardcoded as 'pip install openai==0.28.1' through import_openai()
    # see https://github.com/zilliztech/GPTCache/blob/0.1.44/gptcache/embedding/openai.py#L8
    # because gptcache doesn't have support for openai >=1,
    # see also https://github.com/zilliztech/GPTCache/issues/576
    - openai <1.0.0a0

test:
  imports:
    - gptcache
  commands:
    - pip check
    # Need extra access to install some deps with pip on Windows, like uvicorn, etc.
    - gptcache_server --help  # [not win]
  requires:
    - pip

about:
  home: https://zilliz.com/what-is-gptcache
  summary: GPTCache is a project dedicated to building a semantic cache for storing LLM responses.
  description: |
    GPTCache is a powerful caching library that can be used to speed up and lower the cost of
    chat applications that rely on the LLM service. GPTCache works as a memcache for
    AIGC applications, similar to how Redis works for traditional applications.
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/zilliztech/GPTCache
  doc_url: https://gptcache.readthedocs.io

extra:
  recipe-maintainers:
    - YYYasin19
